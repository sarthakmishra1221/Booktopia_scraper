# -*- coding: utf-8 -*-
"""Booktopia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Up-K06TjMRd_sAAuGioHNDekllr661NK
"""

import requests
import pandas as pd
import json
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import csv

headers = {
    'Host': 'www.booktopia.com.au',
    'user-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0',
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'accept-language': 'en-US,en;q=0.5',
    'upgrade-insecure-requests': '1',
    'sec-fetch-dest': 'document',
    'sec-fetch-mode': 'navigate',
    'sec-fetch-site': 'cross-site',
    'priority': 'u=1',
    'pragma': 'no-cache',
    'cache-control': 'no-cache',
}

def scrape_book_data(isbn):
    try:
        response = requests.get(f'https://www.booktopia.com.au/xx/book/{isbn}.html', headers=headers)
        response.raise_for_status()  # Raises an HTTPError if the status is 4xx or 5xx
        soup = BeautifulSoup(response.text, 'html.parser')
        jsn = json.loads(soup.find('script', type='application/json', id='__NEXT_DATA__').text)

        if len(jsn['props']['pageProps']) > 0:
            product = jsn['props']['pageProps']['product']

            title = product.get('displayName', 'Blank')
            author = ''.join([a['name'] for a in product['contributors'] if a['role'] == 'Author'])
            book_type = product.get('bindingFormat', 'Blank')
            mrp = product.get('retailPrice', 'Blank')
            asp = product.get('salePrice', 'Blank')
            isbn10 = product.get('isbn10', 'Blank')
            publication_date = product.get('publicationDate', 'Blank')
            publisher = product.get('publisher', 'Blank')
            no_of_pages = product.get('numberOfPages', 'Blank')

            return {
                'ISBN13': isbn,
                'Title': title,
                'Author': author,
                'Book Type': book_type,
                'MRP': mrp,
                'ASP': asp,
                'ISBN10': isbn10,
                'Publication Date': publication_date,
                'Publisher': publisher,
                'Number of Pages': no_of_pages
            }
        else:
            print(f'Book with ISBN {isbn} not found')
            return {
                'ISBN13': isbn,
                'Title': 'Book Not Found',
                'Author': '',
                'Book Type': '',
                'MRP': '',
                'ASP': '',
                'ISBN10': '',
                'Publication Date': '',
                'Publisher': '',
                'Number of Pages': ''
            }

    except Exception as e:
        print(f'Error processing ISBN {isbn}: {e}')
        return {
            'ISBN13': isbn,
            'Title': 'Book Not Found',
            'Author': '',
            'Book Type': '',
            'MRP': '',
            'ASP': '',
            'ISBN10': '',
            'Publication Date': '',
            'Publisher': '',
            'Number of Pages': ''
        }

def main():
    df = pd.read_csv('/content/input_list.csv')
    isbns = df['ISBN13'].tolist()

    results = []

    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_isbn = {executor.submit(scrape_book_data, isbn): isbn for isbn in isbns}

        for future in as_completed(future_to_isbn):
            isbn = future_to_isbn[future]
            try:
                data = future.result()
                if data:
                    results.append(data)
            except Exception as e:
                print(f'Error fetching data for ISBN {isbn}: {e}')
    if results:
        df_results = pd.DataFrame(results)
        df_results.to_csv('scraped_books.csv', index=False)

if __name__ == "__main__":
    main()
    print("Scraping completed.")